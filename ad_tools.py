# -*- coding: utf-8 -*-
"""AD_tools.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PZ47ZLkJqk74zJrgv4j_zgZYA0ozXeSL
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline as make_pipeline_imb
from sklearn.model_selection import KFold
from imblearn.over_sampling import RandomOverSampler
from sklearn.decomposition import PCA
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from collections import Counter
import pickle
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
from os import path
import os

debug = 1


def cleanup_data(dataset_name, df, drop, prediction):
    df = df.replace(r'^\s*$', np.nan, regex=True)

    if debug:
        print("in cleanup_data, was passed df with ", df.shape[0], " rows")

    # create feature selection
    columns = ['PTID']
    columns.append(prediction)
    dataset_lower = dataset_name.lower()
    if 'demographic' in dataset_lower:
        columns = columns + ['AGE', 'PTRACCAT',
                             'PTETHCAT', 'PTGENDER', 'PTEDUCAT']
    if 'apoe4' in dataset_lower:
        columns.append('APOE4')
    if 'cogtest' in dataset_lower:
        columns = columns + ['CDRSB', 'ADAS11',
                             'MMSE', 'ADAS13', 'RAVLT_immediate']

    if 'mripct' in dataset_lower:
        columns = columns + ['Ventricles', 'Hippocampus',
                             'WholeBrain', 'Entorhinal', 'MidTemp',
                             'pct_Ventricles', 'pct_Hippocampus',
                             'pct_WholeBrain', 'pct_Entorhinal', 'pct_MidTemp']

    elif 'mri' in dataset_lower:
        columns = columns + ['Ventricles', 'Hippocampus',
                             'WholeBrain', 'Entorhinal', 'MidTemp']

    if 'pet' in dataset_lower:
        columns = columns + ['FDG', 'AV45']
    if 'csf' in dataset_lower:
        columns = columns + ['ABETA_UPENNBIOMK9_04_19_17',
                             'TAU_UPENNBIOMK9_04_19_17', 'PTAU_UPENNBIOMK9_04_19_17']
    if debug:
        print("in cleanup_data grabbing columns: ", columns)

    # Remove diagnosis that are very infrequent
    if prediction == 'DX':
        df = df.loc[(df['DX'] == 'MCI') | (
            df['DX'] == 'Dementia') | (df['DX'] == 'NL')]
    if debug:
        print("in cleanup_data 1")
    elif prediction == 'final_DX':
        df = df.loc[(df['final_DX'] == 'MCI') | (
            df['final_DX'] == 'Dementia') | (df['final_DX'] == 'NL')]

    df = df.loc[:, columns]
    if debug:
        print("in cleanup_data 2, df prevew ", df.head())
    if debug:
        print("df size ", df.shape)

    if drop:
        df.dropna(inplace=True)
    else:
        df.fillna(value=0, inplace=True)

    print("in clean_data 3, dropped all but ", df.shape[0], ' rows')

    if debug:
        print("df preview2: ", df.head(2))

    if debug:
        print(df[prediction].value_counts())
    return df


def create_new_dataset(dataset_name, prediction):
    if prediction == 'DX':
        raw_data = os.path.join('Data', 'raw_data_subset' + '.csv')
    elif prediction == 'final_DX':
        raw_data = os.path.join('Data', 'raw_data_subset_finalDX' + '.csv')
    else:
        print("This diagnosis is not yet supported")

    if debug:
        print("in create_new_dataset: dataset_name, prediction: ",
              dataset_name, prediction)
    filename = os.path.join('Data', dataset_name + '.csv')
    try:
        df = pd.read_csv(raw_data)
        drop = True
        df = cleanup_data(dataset_name, df, drop, prediction)
        # df.sort_values(by='PTID', inplace=True)
        # df.to_csv('testcsv.csv', index=False)
        if df.shape[0]:
            df.to_csv(filename, index=False)
            return 1
        else:
            return 0
    except Exception as e:
        if debug:
            print("in create_new_dataset, error: ", e)
        return 0


def get_data(dataset_name, prediction, **kwargs):

    oversampling = kwargs.get('oversampling')
    scaling = kwargs.get('scaling')
    split = kwargs.get('split')
    scale_X = kwargs.get('scale_X')

    # if len(args):
    #     to_oversample = 1
    #     oversampling = args[0]

    filename = os.path.join('Data', dataset_name + '.csv')

    if debug:
        print("dataset", dataset_name, " exists")

    # X_train, X_test, y_train, y_test = []

    df = pd.read_csv(filename)

    X, y = populate_X_y(df, prediction, ["PTRACCAT", "PTETHCAT", "PTGENDER"])
    columns = X.columns

    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

    if scaling:
        X_train, X_test = scale_features(scaling, X_train, X_test)
    if scale_X:
        X = scale_features(scaling, X)
    if oversampling:
        X_train, y_train = oversample(oversampling, X_train, y_train)

    return X_train, X_test, y_train, y_test, columns, X, y


def dataset_exists(dataset_name):
    if path.isfile(os.path.join('Data', dataset_name + '.csv')):
        return 1
    return 0


def model_exists(model_name):
    if path.isfile(os.path.join('Models', model_name + '.sav')):
        return 1
    return 0


def load_model(model_name):
    # TODO: if model is not dnn load .sav file; otherwise, load dnn file(s)
    model_file = os.path.join('Models', model_name + '.sav')
    print("Looking for file: ", model_file)
    if not 'dnn' in model_name:
        model = pickle.load(open(model_file, 'rb'))
    return model

#
# def build_NN_builder(dim):
#     from keras.models import Sequential
#     from keras.wrappers.scikit_learn import KerasClassifier
#     from keras.callbacks import EarlyStopping
#     from keras.layers import Dense
#
#     def model_no_args():
#         classifier = Sequential()
#         classifier.add(Dense(units=6, activation='relu', input_dim=dim))
#         classifier.add(Dense(units=6, activation='relu'))
#         classifier.add(Dense(units=6, activation='relu'))
#         classifier.add(Dense(units=6, activation='relu'))
#         classifier.add(Dense(units=4, activation='relu'))
#         classifier.add(Dense(units=4, activation='relu'))
#         classifier.add(Dense(units=4, activation='relu'))
#         classifier.add(Dense(units=4, activation='relu'))
#
#         classifier.add(Dense(units=3, activation='softmax'))
#
#         classifier.compile(
#             optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
#         return classifier
#
#     return model_no_args
#
#
# def build_nn_for_skl(dim):
#     from keras.models import Sequential
#     from keras.wrappers.scikit_learn import KerasClassifier
#     from keras.callbacks import EarlyStopping
#     from keras.layers import Dense
#
#     return KerasClassifier(
#         build_NN_builder(dim),
#         epochs=300,
#         shuffle=True,
#         verbose=2,
#         callbacks=[EarlyStopping(monitor='acc', patience=20, verbose=2)]
#     )
#
#


def train_model(model_name, X_train, y_train):
    model_file = os.path.join('Models', model_name + '.sav')
    if debug:
        print("Training new model: ", model_name)
    if 'svc' in model_name.lower():
        model = SVC(kernel='linear', probability=True)

    elif 'random forest' in model_name.lower() or 'rf' in model_name.lower():
        model = RandomForestClassifier(n_estimators=200)

    elif 'logistic regression' in model_name.lower() or 'lr' in model_name.lower():
        model = LogisticRegression()

    elif 'dnn' in model_name.lower():
        model = build_nn_for_skl(len(X_train[0]))

    if debug:
        print("in train_model, X_train looks like this: ", X_train)
    model.fit(X_train, y_train)

    if debug:
        print("saving model to ", model_file)
    if not 'dnn' in model_name.lower():
        pickle.dump(model, open(model_file, 'wb'))

    return model


def cross_validate(model_name, X, y):

    # y = y.reset_index()
    y = y.as_matrix()

    kf = KFold(n_splits=5, random_state=42)
    accuracy = []
    precision = []
    recall = []
    f1 = []
    auc = []

    if debug:
        print("in cross_validate: 1, size of X, y: ", len(X), len(y))
        print("y type is: ", type(y))

    if 'svc' in model_name.lower():
        classifier = SVC(kernel='linear', probability=True)

    elif 'random forest' in model_name.lower() or 'rf' in model_name.lower():
        classifier = RandomForestClassifier(n_estimators=200)

    elif 'logistic regression' in model_name.lower() or 'lr' in model_name.lower():
        classifier = LogisticRegression()

    try:

        for train_indices, test_indices in kf.split(X):

            X_train, X_test = X[train_indices], X[test_indices]
            y_train, y_test = y[train_indices], y[test_indices]
            # y_train, y_test = y[1293:6460], y[0:1292]

            if 'smote' in model_name.lower():
                pipeline = make_pipeline_imb(SMOTE(), classifier)
            else:
                pipeline = make_pipeline_imb(
                    RandomOverSampler(random_state=0), classifier)

            if debug:
                print("in cross_validate: 2, size of X_train, y_train: ",
                      len(X_train), len(y_train))
            #     print("train size, test size: ", len(
            #         train_indices), len(test_indices))
            model = pipeline.fit(X_train, y_train)
            # if debug:
            #     print("pipeline returns: ", pipeline.transform(X_train))
            prediction = model.predict(X_test)

            accuracy.append(pipeline.score(X_test, y_test))
            precision.append(precision_score(
                y_test, prediction, average=None))
            recall.append(recall_score(
                y_test, prediction, average=None))
            f1.append(f1_score(y_test, prediction, average=None))

    except Exception as e:
        print("error in k-fold validate: ", e)
        print("X[train] is: ", X_train)
        print("Y[train] is: ",  y_train)

    print(f"k-fold accuracy: {accuracy}")
    print(f"k-fold recall: {recall}")
    print(f"k-fold precision: {precision}")
    print(f"k-fold f1: {f1}")
    return accuracy, precision, recall, f1


def visualize_tree(model, feature_list):
    # Import tools needed for visualization
    from sklearn.tree import export_graphviz
    import pydot

    # Pull out one tree from the forest
    tree = model.estimators_[0]

    # Export the image to a dot file
    # export_graphviz(tree, out_file='tree.dot',
    #                 feature_names=feature_list, rounded=True, precision=1)

    # Use dot file to create a graph
    # (graph, ) = pydot.graph_from_dot_file('tree.dot')

    # Write graph to a png file
    # os.path.join('Data', 'raw_data_subset' + '.csv')
    # graph.write_png(os.path.join('static', 'images', 'tree.png'))
    # print('writing ', os.path.join('static', 'images', 'tree.png'))


def eval_and_report(model, X_test, y_test, size, X_features, X, y, *args):
    cv = 0

    cv_accuracy_arr = []

    if len(args):
        cv = 1
        cv_accuracy_arr = args[0]

    try:
        metrics = evaluate_model(
            model, X_test, y_test, X_features, X, y)
        class_report = metrics['class_report']
        if cv:
            score = round(np.mean(cv_accuracy_arr), 4)
        else:
            score = metrics['score']
        features = metrics['features']

        layout = {
            'title': {
                'text': 'Score: ' + str(score)
                + '<br>Training set size: ' + str(size)
            },
            'xaxis': {
                'title': {
                    'text': 'False Positive Rate'
                }
            },
            'yaxis': {
                'title': {
                    'text': 'True Positive Rate'
                }
            }
        }

        data = [
            {
                "x": metrics['fpr'][0],
                "y": metrics['tpr'][0],
                "name":'Dementia ROC curve (area:' + str(metrics['roc_auc'][0]) + ')'
            },
            {
                "x": metrics['fpr'][1],
                "y": metrics['tpr'][1],
                "name":'MCI ROC curve (area:' + str(metrics['roc_auc'][1]) + ')'
            },
            {
                "x": metrics['fpr'][2],
                "y": metrics['tpr'][2],
                "name":'NL ROC curve (area:' + str(metrics['roc_auc'][2]) + ')'
            },
        ]

        cv_data = [
            {
                "y": cv_accuracy_arr,
                'type': 'box',
                "name":  "K - fold Validation Accuracy (K=5)"
            }
        ]

        response = {
            'data': data,
            'cv': cv,
            'cvaccdata': cv_data,
            'score': score,
            'class_report': class_report,
            'features': features,
            'size': size,
            'layout': layout,
            'success': 1
        }

    except Exception as e:
        print("inside eval_and_report, issue with evaluating model", e)
        response = {
            'success': 0
        }
    try:
        visualize_tree(model, X_features)
    except Exception as e:
        print('error saving png file: ', e)
    # print("returning to js: ", jsonify(response))
    return (response)


def evaluate_model(model, X_test, y_test, X_features, X, y):

    score = round(model.score(X_test, y_test), 4)
    predictions = model.predict(X_test)
    confmatrix = confusion_matrix(y_test, predictions)
    class_report = classification_report(y_test, predictions, output_dict=True)
    roc_auc, fpr, tpr = return_roc(y_test, model.predict_proba(X_test))
    try:
        feature_importances = model.feature_importances_
        features = sorted(
            zip(feature_importances, X_features), reverse=False)
        # print("got features: ", features)
    except:
        print("could not find feature_importances_ in model ", model)
        features = [(-1, -1)]

    metrics = {
        'score': score,
        'class_report': class_report,
        'roc_auc': roc_auc,
        'features': dict(features),
        'fpr': fpr,
        'tpr': tpr
    }
    return metrics


def return_roc(y_test, y_score):
    classes = ['Dementia', 'MCI', 'NL']
    y_bin = label_binarize(y_test, classes=['Dementia', 'MCI', 'NL'])
    n_classes = 3
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_score[:, i])
        roc_auc[i] = round(auc(fpr[i], tpr[i]), 4)
        fpr[i] = fpr[i].tolist()
        tpr[i] = tpr[i].tolist()
        # roc_auc[i]
    return roc_auc, fpr, tpr


def add_pct_change(df, feature_list):
    for feature in feature_list:
        new_column = "pct_" + feature
#     df[feature].loc[df[feature] == 0][feature] = 1
#     if divisor != 0:
        df[new_column] = 100 * (1 - df.iloc[0][feature] / df[feature])
    return df


def add_sum_column(df):
    df['pct_sum'] = df['pct_Hippocampus'] + df['pct_WholeBrain'] + \
        df['pct_Entorhinal'] + df['pct_MidTemp']
    return df


def return_model_name(dict):
    tag_elements = []
    for key in dict.keys():
        if (key != 'cross_validate'):
            tag_elements.append(dict[key])
    tag = '_'.join(tag_elements)
    return tag


def get_dataset_name(dict):
    tag_elements = []
    for key in dict.keys():
        if (key != 'model') and (key != 'oversampling') and (key != 'scaling') and (key != 'cross_validate'):
            tag_elements.append(dict[key])
    tag = '_'.join(tag_elements)
    return tag


def add_final_dx_column(df):
    for dx in ['Dementia', 'MCI', "NL"]:
        if dx in df['DX'].values:
            df['final_DX'] = dx
            return df
    return df


def show_data(df, category):
    print("There are ", df[category].count(), " records")
    print("Category breakdown \n", df[category].value_counts())


def populate_X_y(df, y_cat, encode_list):
    y = df[y_cat]
    X = df.drop(columns=[y_cat, 'PTID'])
    label_encoder = LabelEncoder()
    for label in encode_list:
        if label in X:
            encode = X[label]
            label_encoder.fit(encode)
            X[label] = label_encoder.transform(encode)

    return X, y


def myPCA(X_train, X_test):
    pca = PCA()
    X_train = pca.fit_transform(X_train)
    X_test = pca.transform(X_test)
    return X_train, X_test


def scale_features(scaler, X_train, *args):

    X_test_exists = 0

    if len(args):
        X_test = args[0]
        X_test_exists = 1

    if scaler == 'minmax':
        X_train = MinMaxScaler().fit(X_train).transform(X_train)
        if X_test_exists:
            X_test = MinMaxScaler().fit(X_test).transform(X_test)
    elif scaler == 'standard':
        X_train = StandardScaler().fit(X_train).transform(X_train)
        if X_test_exists:
            X_test = StandardScaler().fit(X_test).transform(X_test)

    if X_test_exists:
        return X_train, X_test
    else:
        return X_train


def oversample(alg, X_train, y_train):
    # print('in oversample got ', X_train, y_train)
    if alg == 'smote':
        smt = SMOTE()
        X_train, y_train = smt.fit_sample(X_train, y_train)
        # print('Resampled dataset shape %s' % Counter(y_train))
        return X_train, y_train
    if alg == 'random':
        ros = RandomOverSampler(random_state=42)
        X_train, y_train = ros.fit_sample(X_train, y_train)
        # print('Resampled dataset shape %s' % Counter(y_train))
        return X_train, y_train

# def save_model()
